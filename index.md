# Sleep-Consolidated Learning and Plasticity Decay
## Introduction

## Methods

## Results

## Conclusion

## Limitations and Future Work

## References
1. Aaron Grattafiori et al.. 2024. “The Llama 3 Herd of Models.” [Link]
2. Balduzzi, D., and G. Tononi. 2013. “What can neurons do for their brain? Communicate selectivity with bursts.” Theory Biosci 132(1): 27–39. [Link]
3. Hart, Betty, and Todd R. Risley. 1992. “American Parenting of Language-Learning Children: Persisting Differences in Family-Child Interactions Observed in Natural Home Environments..” Developmental Psychology 28: 1096–1105. [Link]
4. Hu, Michael Y., Aaron Mueller, Candace Ross, Adina Williams, Tal Linzen, Chengxu Zhuang, Ryan Cotterell, Leshem Choshen, Alex Warstadt, and Ethan Gotlieb Wilcox. 2024. “Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora.” [Link]
5. Marr, D. 1971. “Simple memory: a theory for archicortex.” Philosophical Transactions of the Royal Society of London. B, Biological Sciences 262(841): 23–81. [Link]
6. Ott, Myle, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. “fairseq: A Fast, Extensible Toolkit for Sequence Modeling.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). Minneapolis, Minnesota Association for Computational Linguistics. [Link]
7. Oudiette, Delphine, and Ken A. Paller. 2013. “Upgrading the sleeping brain with targeted memory reactivation.” Trends in Cognitive Sciences 17(3): 142–149. [Link] 
8. Rasch, Björn, and Jan Born. 2013. “About Sleep’s Role in Memory.” Physiological Reviews 93(2): 681–766. [Link]
9. Tadros, Timothy, Giri P. Krishnan, Ramyaa Ramyaa, and Maxim Bazhenov. 2022. “Sleep-like unsupervised replay reduces catastrophic forgetting in artificial neural networks.” Nature Communications 13(1), p. 7742. [Link]
10. Tononi, Giulio, and Chiara Cirelli. 2014. “Sleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration.” Neuron 81(1): 12–34. [Link]
11. Warstadt, Alex, and Samuel R. Bowman. 2024. “What Artificial Neural Networks Can Tell Us About Human Language Acquisition.” [Link]
12. Warstadt, Alex, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell. 2023. “Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora.” In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Association for Computational Linguistics. [Link]
13. Xiao, Chenghao, G Thomas Hudson, and Noura Al Moubayed. 2023. “Towards more Human-like Language Models based on Contextualizer Pretraining Strategy.” In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Singapore Association for Computational Linguistics. [Link]